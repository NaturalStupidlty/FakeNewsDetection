{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-23T14:48:21.202905Z",
     "iopub.status.busy": "2023-02-23T14:48:21.202384Z",
     "iopub.status.idle": "2023-02-23T14:48:21.216887Z",
     "shell.execute_reply": "2023-02-23T14:48:21.215544Z",
     "shell.execute_reply.started": "2023-02-23T14:48:21.202847Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          logging)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\"\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T14:48:21.220463Z",
     "iopub.status.busy": "2023-02-23T14:48:21.219653Z",
     "iopub.status.idle": "2023-02-23T14:48:21.237113Z",
     "shell.execute_reply": "2023-02-23T14:48:21.235994Z",
     "shell.execute_reply.started": "2023-02-23T14:48:21.220424Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_COLUMNS = [\"toxic\",\n",
    "                  \"severe_toxic\",\n",
    "                  \"obscene\",\n",
    "                  \"threat\",\n",
    "                  \"insult\",\n",
    "                  \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T14:48:21.241930Z",
     "iopub.status.busy": "2023-02-23T14:48:21.241466Z",
     "iopub.status.idle": "2023-02-23T14:48:21.265972Z",
     "shell.execute_reply": "2023-02-23T14:48:21.265011Z",
     "shell.execute_reply.started": "2023-02-23T14:48:21.241890Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^\\w\\s]','', text) \n",
    "        # Remove non-ASCII characters (such as emojis)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text) \n",
    "        text = \" \".join(tokenize.sent_tokenize(text))\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"\\n+\", \". \", text)\n",
    "        for symb in [\"!\", \",\", \":\", \";\", \"?\"]:\n",
    "            text = re.sub(rf\"\\{symb}\\.\", symb, text)\n",
    "        text = re.sub(\"[^а-яА-Яa-zA-Z0-9!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ё]+\", \" \", text)\n",
    "        text = re.sub(r\"#\\S+\", \"\", text)\n",
    "        text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_sequential_dots(text):\n",
    "    # Collapse sequential dots\n",
    "    text = re.sub(\"\\.+\", \".\", text)\n",
    "    # Collapse dots separated by whitespaces\n",
    "    all_collapsed = False\n",
    "    while not all_collapsed:\n",
    "        output = re.sub(r\"\\.(( )*)\\.\", \".\", text)\n",
    "        all_collapsed = text == output\n",
    "        text = output\n",
    "    return output\n",
    "\n",
    "def remove_stop_words(text, stop_words):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def preprocess_text(text, stop_words=stopwords.words('english')):\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_sequential_dots(text)\n",
    "    text = remove_stop_words(text, stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T14:48:21.269636Z",
     "iopub.status.busy": "2023-02-23T14:48:21.269157Z",
     "iopub.status.idle": "2023-02-23T14:48:21.281515Z",
     "shell.execute_reply": "2023-02-23T14:48:21.280393Z",
     "shell.execute_reply.started": "2023-02-23T14:48:21.269597Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(train,\n",
    "               test,\n",
    "               test_labels,\n",
    "               max_text_length = 512,\n",
    "               sample=1):\n",
    "    test = test.merge(test_labels, on=\"id\")\n",
    "    \n",
    "    train = train.sample(int(train.shape[0]*sample))\n",
    "    test = test.sample(int(test.shape[0]*sample))\n",
    "    \n",
    "    #closed_test = test[(test[TARGET_COLUMNS] == -1).any(axis=1)].reset_index(drop=True)\n",
    "    test = test[~(test[TARGET_COLUMNS] == -1).any(axis=1)].reset_index(drop=True)\n",
    "    \n",
    "    train['clean_comment_text'] = train['comment_text'].apply(preprocess_text)\n",
    "    test['clean_comment_text'] = test['comment_text'].apply(preprocess_text)\n",
    "    \n",
    "    train['clean_comment_text'] = train['clean_comment_text'].str.slice(0, max_text_length)\n",
    "    test['clean_comment_text'] = test['clean_comment_text'].str.slice(0, max_text_length)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T14:48:21.283201Z",
     "iopub.status.busy": "2023-02-23T14:48:21.282868Z",
     "iopub.status.idle": "2023-02-23T14:48:21.296988Z",
     "shell.execute_reply": "2023-02-23T14:48:21.295768Z",
     "shell.execute_reply.started": "2023-02-23T14:48:21.283154Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def comp_metric(y_true, y_pred, verbose=1):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    class_roc_aucs = [roc_auc_score(y_true[:,i], y_pred[:,i]) for i in range(y_pred.shape[1])]\n",
    "    if verbose:\n",
    "        for ra, tgt_col in zip(class_roc_aucs, TARGET_COLUMNS):\n",
    "            if verbose > 1:\n",
    "                print(f\"{tgt_col} Roc Auc: {ra}\")\n",
    "        print(f\"Result Roc Auc: {np.mean(class_roc_aucs)}\")\n",
    "    return class_roc_aucs, np.mean(class_roc_aucs)\n",
    "\n",
    "def print_losses(input, verbose=1):\n",
    "    if verbose:\n",
    "        for cls_idx, cls_name in enumerate(TARGET_COLUMNS):\n",
    "            if verbose > 1:\n",
    "                print(f\"{cls_name} BCE loss: {input[:,cls_idx].mean()}\")\n",
    "        print(f\"Result BCE loss: {input.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T14:48:21.301162Z",
     "iopub.status.busy": "2023-02-23T14:48:21.300227Z",
     "iopub.status.idle": "2023-02-23T14:48:21.314601Z",
     "shell.execute_reply": "2023-02-23T14:48:21.313599Z",
     "shell.execute_reply.started": "2023-02-23T14:48:21.301124Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts,\n",
    "        targets,\n",
    "        dataset_tokenizer,\n",
    "        max_length,\n",
    "        trim_policy=\"random\"\n",
    "    ):\n",
    "        self.targets = targets\n",
    "        self.tokenized_texts =  dataset_tokenizer(texts)\n",
    "        self.tokenizer = dataset_tokenizer\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        if trim_policy not in [\"random\", \"first\"]:\n",
    "            raise ValueError(f\"{trim_policy} is not valid trim_policy\")\n",
    "        self.trim_policy = trim_policy\n",
    "        \n",
    "    def select_text_subsequance(self, input):\n",
    "        input_len = len(input[\"input_ids\"])\n",
    "        if input_len < self.max_length:\n",
    "            pad_len = self.max_length - input_len\n",
    "            return {\n",
    "                \"input_ids\": input[\"input_ids\"] + [self.tokenizer.pad_token_id] * pad_len,\n",
    "                \"attention_mask\": input[\"attention_mask\"] + [0] * pad_len\n",
    "            } \n",
    "        elif input_len > self.max_length:\n",
    "            if self.trim_policy == \"random\":\n",
    "                start = np.random.randint(0, input_len - self.max_length)\n",
    "            elif self.trim_policy == \"first\":\n",
    "                start = 0\n",
    "            return {\n",
    "                \"input_ids\": input[\"input_ids\"][start : start + self.max_length - 1] + [self.tokenizer.sep_token_id] ,\n",
    "                \"attention_mask\": input[\"attention_mask\"][start : start + self.max_length]\n",
    "            }\n",
    "        else: \n",
    "            return input\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized = {k:v[idx] for k,v in self.tokenized_texts.items()}\n",
    "        tokenized = self.select_text_subsequance(tokenized)\n",
    "        tokenized = {k:torch.LongTensor(v) for k,v in tokenized.items()}\n",
    "        tokenized[\"target\"] = torch.from_numpy(self.targets[idx]).float()\n",
    "        return tokenized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T14:48:21.317839Z",
     "iopub.status.busy": "2023-02-23T14:48:21.316894Z",
     "iopub.status.idle": "2023-02-23T14:48:21.363379Z",
     "shell.execute_reply": "2023-02-23T14:48:21.362123Z",
     "shell.execute_reply.started": "2023-02-23T14:48:21.317721Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, train_dataset, test_dataset):\n",
    "        # Datasets\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "        # Model\n",
    "        self.model = {}\n",
    "        self.is_built = False\n",
    "        self.is_trained = False\n",
    "        self.n_epochs = None\n",
    "        self.train_torch_dataloader = {}\n",
    "        self.valid_torch_dataloader = {}\n",
    "        self.test_torch_dataloader = {}\n",
    "        self.tokenizer = {}\n",
    "        self.optimizer = {}\n",
    "        self.criterion = {}\n",
    "        self.scheduler = {}\n",
    "        self.batch_size = None\n",
    "        self.max_length = None\n",
    "        self.gradient_clipping = None\n",
    "        self.lr_update = None\n",
    "        self.num_workers = None\n",
    "        self.device = \"\"\n",
    "        \n",
    "        # Train results\n",
    "        self.train_all_epoch_labels = []\n",
    "        self.train_all_epoch_losses = []\n",
    "        self.train_all_epoch_targets = []\n",
    "        self.valid_all_epoch_labels = []\n",
    "        self.valid_all_epoch_losses = []\n",
    "        self.valid_all_epoch_targets = []\n",
    "        self.valid_roc_aucs = []\n",
    "        self.train_roc_aucs = []\n",
    "        self.best_metric = - np.inf\n",
    "        self.best_model_state_dict = None\n",
    "        \n",
    "    # Prepare evething and initialize the model\n",
    "    def build(self,\n",
    "              tokenizer_name='bert-base-uncased',\n",
    "              model_name='bert-base-uncased',\n",
    "              max_length=256,\n",
    "              batch_size=32,\n",
    "              FOLDS=5,\n",
    "              n_epochs=5,\n",
    "              gradient_clipping=False,\n",
    "              lr_update=False,\n",
    "              num_workers=2,\n",
    "              device=\"cuda\"):\n",
    "        self.is_built = True\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.max_length = max_length\n",
    "        self.lr_update = lr_update\n",
    "        self.gradient_clipping = gradient_clipping\n",
    "        self.num_workers = num_workers\n",
    "        self.device = device\n",
    "        \n",
    "        # Create stritified target from target columns\n",
    "        train[\"stratified_target\"] = train[TARGET_COLUMNS].apply(\n",
    "            lambda x: reduce(lambda x, y: str(x) + str(y), x), axis=1)\n",
    "        small_groups = train[\"stratified_target\"].value_counts()[\n",
    "            train[\"stratified_target\"].value_counts() < FOLDS].index\n",
    "        train.loc[train[\"stratified_target\"].isin(small_groups), \"stratified_target\"] = \"-1\"\n",
    "        \n",
    "        # Cross-validation\n",
    "        stratifier = StratifiedKFold(n_splits=FOLDS,\n",
    "                                     random_state=69,\n",
    "                                     shuffle=True)\n",
    "        folds_ids = [el for el in stratifier.split(train,\n",
    "                                                   train[\"stratified_target\"])]\n",
    "        \n",
    "        # Tokenize text and create dataloader\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        train_torch_dataset = TextDataset(\n",
    "            texts=train.iloc[folds_ids[0][0]][\"clean_comment_text\"].to_list(),\n",
    "            targets=train.iloc[folds_ids[0][0]][TARGET_COLUMNS].values,\n",
    "            dataset_tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        self.train_torch_dataloader = torch.utils.data.DataLoader(\n",
    "            train_torch_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        valid_torch_dataset = TextDataset(\n",
    "            texts=train.iloc[folds_ids[0][1]][\"clean_comment_text\"].to_list(),\n",
    "            targets=train.iloc[folds_ids[0][1]][TARGET_COLUMNS].values,\n",
    "            dataset_tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length,\n",
    "            trim_policy=\"first\"\n",
    "        )\n",
    "        self.valid_torch_dataloader = torch.utils.data.DataLoader(\n",
    "            valid_torch_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(TARGET_COLUMNS), \n",
    "            ignore_mismatched_sizes=True,\n",
    "            max_length=self.max_length\n",
    "        ).to(device)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': self.model.bert.parameters(), \"lr\": 1e-5},\n",
    "            {'params': self.model.classifier.parameters(), \"lr\": 1e-3},],\n",
    "            weight_decay=0,\n",
    "            amsgrad=True\n",
    "        )\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=self.optimizer,\n",
    "            num_warmup_steps=int(0.05*len(self.train_torch_dataloader) * self.n_epochs),\n",
    "            num_training_steps=len(self.train_torch_dataloader) * self.n_epochs\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def train(self, verbose=1):\n",
    "        if not self.is_built:\n",
    "            raise Exception('The model was not build and cannot be trained!')\n",
    "        \n",
    "        self.is_trained = True\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # 1.1 Iterate over all train dataset and update model weights\n",
    "            if verbose:\n",
    "                print(f\"Starting Epoch {epoch + 1}\")\n",
    "                print(\"Train phase\")\n",
    "            (train_epoch_labels,\n",
    "            train_epoch_losses,\n",
    "            train_epoch_targets) = self.torch_loop( \n",
    "                mode=\"train\"\n",
    "            )\n",
    "            # 1.2 Compute and print train metrics\n",
    "            if verbose:\n",
    "                print(\"Train metrics\")\n",
    "            _, train_roc_auc = comp_metric(\n",
    "                train_epoch_targets, \n",
    "                train_epoch_labels,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            if verbose:\n",
    "                print(\"Train BCE losses\")\n",
    "            print_losses(train_epoch_losses)\n",
    "            # 2.1 Iterate over all valid dataset and compute predictions\n",
    "            if verbose:\n",
    "                print(\"Valid phase\")\n",
    "            (valid_epoch_labels,\n",
    "            valid_epoch_losses,\n",
    "            valid_epoch_targets) = self.torch_loop(\n",
    "                validation=True, \n",
    "                mode=\"eval\"\n",
    "            )\n",
    "            # 2.2 Compute and print valid metrics\n",
    "            if verbose:\n",
    "                print(\"Valid metrics\")\n",
    "            _, valid_roc_auc = comp_metric(\n",
    "                valid_epoch_targets, \n",
    "                valid_epoch_labels,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            if verbose:\n",
    "                print(\"Valid BCE losses\")\n",
    "            print_losses(valid_epoch_losses, verbose=verbose)\n",
    "            # 3. Update learning rate\n",
    "            if self.lr_update:\n",
    "                self.scheduler.step(valid_roc_auc)\n",
    "            # 4. Save best model\n",
    "            if valid_roc_auc > self.best_metric:\n",
    "                self.best_metric = valid_roc_auc\n",
    "                self.best_model_state_dict = deepcopy(self.model.state_dict())\n",
    "            # 5. Accumulate all stats  \n",
    "            self.train_all_epoch_labels.append(train_epoch_labels)\n",
    "            self.train_all_epoch_losses.append(train_epoch_losses)\n",
    "            self.train_all_epoch_targets.append(train_epoch_targets)\n",
    "            self.valid_all_epoch_labels.append(valid_epoch_labels)\n",
    "            self.valid_all_epoch_losses.append(valid_epoch_losses)\n",
    "            self.valid_all_epoch_targets.append(valid_epoch_targets)\n",
    "            self.valid_roc_aucs.append(valid_roc_auc)\n",
    "            self.train_roc_aucs.append(train_roc_auc)\n",
    "        return self\n",
    "    \n",
    "    def torch_loop(self,\n",
    "                   validation=False,\n",
    "                   mode=\"train\"):\n",
    "        if mode == \"train\":\n",
    "            self.model.train()\n",
    "            dataloader = self.train_torch_dataloader\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            if validation:\n",
    "                dataloader = self.valid_torch_dataloader\n",
    "            else:\n",
    "                dataloader = self.test_torch_dataloader\n",
    "    \n",
    "        all_predicted_label = []\n",
    "        all_losses = []\n",
    "        all_targets = []\n",
    "        with torch.inference_mode(mode=(mode != \"train\")):\n",
    "            for text in tqdm(dataloader):\n",
    "                text = {k:v.to(self.device) for k,v in text.items()}\n",
    "                label = text.pop(\"target\")\n",
    "                if mode == \"train\":\n",
    "                    self.optimizer.zero_grad()\n",
    "                predicted_label = self.model(**text).logits\n",
    "                loss = self.criterion(predicted_label, label)\n",
    "                if mode == \"train\":\n",
    "                    loss.mean().backward()\n",
    "                    if self.gradient_clipping:\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n",
    "                    self.optimizer.step()\n",
    "                    if self.scheduler is not None:\n",
    "                        self.scheduler.step()\n",
    "\n",
    "                all_predicted_label.append(torch.sigmoid(predicted_label.detach()).cpu().numpy())\n",
    "                all_losses.append(loss.detach().cpu().numpy())\n",
    "                all_targets.append(label.detach().cpu().numpy())\n",
    "        all_predicted_label = np.concatenate(all_predicted_label)\n",
    "        all_losses = np.concatenate(all_losses)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "\n",
    "        return all_predicted_label, all_losses, all_targets\n",
    "    \n",
    "    def evaluate(self, verbose=1):\n",
    "        if not self.is_trained:\n",
    "            raise Exception('The model was not trained and cannot be evaluated!')\n",
    "            return self\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(self.best_model_state_dict)\n",
    "        test_torch_dataset = TextDataset(\n",
    "            texts=test[\"clean_comment_text\"].to_list(),\n",
    "            targets=test[TARGET_COLUMNS].values,\n",
    "            dataset_tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length,\n",
    "            trim_policy=\"first\"\n",
    "        )\n",
    "        self.test_torch_dataloader = torch.utils.data.DataLoader(\n",
    "            test_torch_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        test_labels, test_losses, test_targets = self.torch_loop( \n",
    "            mode=\"eval\"\n",
    "        )\n",
    "        print(\"Test metrics\")\n",
    "        comp_metric(\n",
    "            test_targets, \n",
    "            test_labels,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        print(\"Test BCE losses\")\n",
    "        print_losses(test_losses, verbose=verbose)\n",
    "        \n",
    "        return comp_metric(test_targets,\n",
    "                           test_labels,\n",
    "                           verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T14:48:21.366594Z",
     "iopub.status.busy": "2023-02-23T14:48:21.365962Z",
     "iopub.status.idle": "2023-02-23T19:24:38.525646Z",
     "shell.execute_reply": "2023-02-23T19:24:38.524420Z",
     "shell.execute_reply.started": "2023-02-23T14:48:21.366556Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Train phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [49:23<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Result Roc Auc: 0.9528606335339806\n",
      "Train BCE losses\n",
      "Result BCE loss: 0.0667518675327301\n",
      "Valid phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [03:58<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid metrics\n",
      "Result Roc Auc: 0.9811179724589462\n",
      "Valid BCE losses\n",
      "Result BCE loss: 0.04476262256503105\n",
      "Starting Epoch 2\n",
      "Train phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [49:28<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Result Roc Auc: 0.985439725569733\n",
      "Train BCE losses\n",
      "Result BCE loss: 0.04045988991856575\n",
      "Valid phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [03:58<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid metrics\n",
      "Result Roc Auc: 0.9840558075407292\n",
      "Valid BCE losses\n",
      "Result BCE loss: 0.04619410261511803\n",
      "Starting Epoch 3\n",
      "Train phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [49:28<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Result Roc Auc: 0.9904272472914948\n",
      "Train BCE losses\n",
      "Result BCE loss: 0.03487158566713333\n",
      "Valid phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [49:25<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Result Roc Auc: 0.9932440832283023\n",
      "Train BCE losses\n",
      "Result BCE loss: 0.030269404873251915\n",
      "Valid phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [03:58<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid metrics\n",
      "Result Roc Auc: 0.9836613421122458\n",
      "Valid BCE losses\n",
      "Result BCE loss: 0.046889182180166245\n",
      "Starting Epoch 5\n",
      "Train phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [49:20<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Result Roc Auc: 0.9945594680559883\n",
      "Train BCE losses\n",
      "Result BCE loss: 0.027213888242840767\n",
      "Valid phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [03:57<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid metrics\n",
      "Result Roc Auc: 0.9824779298619396\n",
      "Valid BCE losses\n",
      "Result BCE loss: 0.04867338761687279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [07:55<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics\n",
      "Result Roc Auc: 0.9812361781522636\n",
      "Test BCE losses\n",
      "Result BCE loss: 0.07720058411359787\n",
      "Result Roc Auc: 0.9812361781522636\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n",
    "test_labels = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\")\n",
    "    \n",
    "train, test = preprocess(train,\n",
    "                         test,\n",
    "                         test_labels,\n",
    "                         max_length=256,\n",
    "                         batch_size=32)\n",
    "\n",
    "model = Model(train, test)\n",
    "model.build().train().evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
