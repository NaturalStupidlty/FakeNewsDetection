{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-27T13:45:48.879868Z",
     "iopub.status.busy": "2023-02-27T13:45:48.879361Z",
     "iopub.status.idle": "2023-02-27T13:45:48.891233Z",
     "shell.execute_reply": "2023-02-27T13:45:48.890085Z",
     "shell.execute_reply.started": "2023-02-27T13:45:48.879829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from nltk import tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          logging)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\"\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T13:45:48.894161Z",
     "iopub.status.busy": "2023-02-27T13:45:48.893645Z",
     "iopub.status.idle": "2023-02-27T13:45:48.909695Z",
     "shell.execute_reply": "2023-02-27T13:45:48.908627Z",
     "shell.execute_reply.started": "2023-02-27T13:45:48.894125Z"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_COLUMNS = [\"toxic\",\n",
    "                  \"severe_toxic\",\n",
    "                  \"obscene\",\n",
    "                  \"threat\",\n",
    "                  \"insult\",\n",
    "                  \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T13:45:48.913089Z",
     "iopub.status.busy": "2023-02-27T13:45:48.912778Z",
     "iopub.status.idle": "2023-02-27T13:45:48.925955Z",
     "shell.execute_reply": "2023-02-27T13:45:48.925026Z",
     "shell.execute_reply.started": "2023-02-27T13:45:48.913061Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^\\w\\s]','', text) \n",
    "        # Remove non-ASCII characters (such as emojis)\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text) \n",
    "        text = \" \".join(tokenize.sent_tokenize(text))\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"\\n+\", \". \", text)\n",
    "        for symb in [\"!\", \",\", \":\", \";\", \"?\"]:\n",
    "            text = re.sub(rf\"\\{symb}\\.\", symb, text)\n",
    "        text = re.sub(\"[^а-яА-Яa-zA-Z0-9!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ё]+\", \" \", text)\n",
    "        text = re.sub(r\"#\\S+\", \"\", text)\n",
    "        text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_sequential_dots(text):\n",
    "    # Collapse sequential dots\n",
    "    text = re.sub(\"\\.+\", \".\", text)\n",
    "    # Collapse dots separated by whitespaces\n",
    "    all_collapsed = False\n",
    "    while not all_collapsed:\n",
    "        output = re.sub(r\"\\.(( )*)\\.\", \".\", text)\n",
    "        all_collapsed = text == output\n",
    "        text = output\n",
    "    return output\n",
    "\n",
    "def remove_stop_words(text, stop_words):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def preprocess_text(text, stop_words=stopwords.words('english')):\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_sequential_dots(text)\n",
    "    text = remove_stop_words(text, stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T13:45:48.929070Z",
     "iopub.status.busy": "2023-02-27T13:45:48.928657Z",
     "iopub.status.idle": "2023-02-27T13:45:48.940550Z",
     "shell.execute_reply": "2023-02-27T13:45:48.939653Z",
     "shell.execute_reply.started": "2023-02-27T13:45:48.929034Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(train,\n",
    "               test,\n",
    "               test_labels,\n",
    "               max_text_length = 512,\n",
    "               sample=1):\n",
    "    test = test.merge(test_labels, on=\"id\")\n",
    "    \n",
    "    train = train.sample(int(train.shape[0]*sample), random_state=69)\n",
    "    test = test.sample(int(test.shape[0]*sample), random_state=69)\n",
    "    \n",
    "    #closed_test = test[(test[TARGET_COLUMNS] == -1).any(axis=1)].reset_index(drop=True)\n",
    "    test = test[~(test[TARGET_COLUMNS] == -1).any(axis=1)].reset_index(drop=True)\n",
    "    \n",
    "    train['clean_comment_text'] = train['comment_text'].apply(preprocess_text)\n",
    "    test['clean_comment_text'] = test['comment_text'].apply(preprocess_text)\n",
    "    \n",
    "    train['clean_comment_text'] = train['clean_comment_text'].str.slice(0, max_text_length)\n",
    "    test['clean_comment_text'] = test['clean_comment_text'].str.slice(0, max_text_length)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T13:45:48.942433Z",
     "iopub.status.busy": "2023-02-27T13:45:48.941660Z",
     "iopub.status.idle": "2023-02-27T13:45:48.956410Z",
     "shell.execute_reply": "2023-02-27T13:45:48.955430Z",
     "shell.execute_reply.started": "2023-02-27T13:45:48.942392Z"
    }
   },
   "outputs": [],
   "source": [
    "def comp_metric(y_true, y_pred, verbose=1):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    class_roc_aucs = [roc_auc_score(y_true[:,i], y_pred[:,i]) for i in range(y_pred.shape[1])]\n",
    "    if verbose:\n",
    "        for ra, tgt_col in zip(class_roc_aucs, TARGET_COLUMNS):\n",
    "            if verbose > 1:\n",
    "                print(f\"{tgt_col} Roc Auc: {ra}\")\n",
    "        print(f\"Result Roc Auc: {np.mean(class_roc_aucs)}\")\n",
    "    return class_roc_aucs, np.mean(class_roc_aucs)\n",
    "\n",
    "def print_losses(input, verbose=1):\n",
    "    if verbose:\n",
    "        for cls_idx, cls_name in enumerate(TARGET_COLUMNS):\n",
    "            if verbose > 1:\n",
    "                print(f\"{cls_name} BCE loss: {input[:,cls_idx].mean()}\")\n",
    "        print(f\"Result BCE loss: {input.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T13:45:48.959757Z",
     "iopub.status.busy": "2023-02-27T13:45:48.958679Z",
     "iopub.status.idle": "2023-02-27T13:45:48.972828Z",
     "shell.execute_reply": "2023-02-27T13:45:48.971890Z",
     "shell.execute_reply.started": "2023-02-27T13:45:48.959711Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts,\n",
    "        targets,\n",
    "        dataset_tokenizer,\n",
    "        max_length,\n",
    "        trim_policy=\"random\"\n",
    "    ):\n",
    "        self.targets = targets\n",
    "        self.tokenized_texts =  dataset_tokenizer(texts)\n",
    "        self.tokenizer = dataset_tokenizer\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        if trim_policy not in [\"random\", \"first\"]:\n",
    "            raise ValueError(f\"{trim_policy} is not valid trim_policy\")\n",
    "        self.trim_policy = trim_policy\n",
    "        \n",
    "    def select_text_subsequance(self, input):\n",
    "        input_len = len(input[\"input_ids\"])\n",
    "        if input_len < self.max_length:\n",
    "            pad_len = self.max_length - input_len\n",
    "            return {\n",
    "                \"input_ids\": input[\"input_ids\"] + [self.tokenizer.pad_token_id] * pad_len,\n",
    "                \"attention_mask\": input[\"attention_mask\"] + [0] * pad_len\n",
    "            } \n",
    "        elif input_len > self.max_length:\n",
    "            if self.trim_policy == \"random\":\n",
    "                start = np.random.randint(0, input_len - self.max_length)\n",
    "            elif self.trim_policy == \"first\":\n",
    "                start = 0\n",
    "            return {\n",
    "                \"input_ids\": input[\"input_ids\"][start : start + self.max_length - 1] + [self.tokenizer.sep_token_id] ,\n",
    "                \"attention_mask\": input[\"attention_mask\"][start : start + self.max_length]\n",
    "            }\n",
    "        else: \n",
    "            return input\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized = {k:v[idx] for k,v in self.tokenized_texts.items()}\n",
    "        tokenized = self.select_text_subsequance(tokenized)\n",
    "        tokenized = {k:torch.LongTensor(v) for k,v in tokenized.items()}\n",
    "        tokenized[\"target\"] = torch.from_numpy(self.targets[idx]).float()\n",
    "        return tokenized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T13:45:49.016323Z",
     "iopub.status.busy": "2023-02-27T13:45:49.015592Z",
     "iopub.status.idle": "2023-02-27T13:45:49.058917Z",
     "shell.execute_reply": "2023-02-27T13:45:49.057925Z",
     "shell.execute_reply.started": "2023-02-27T13:45:49.016284Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, train_dataset, test_dataset):\n",
    "        # Datasets\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "        # Model\n",
    "        self.model = {}\n",
    "        self.is_built = False\n",
    "        self.is_trained = False\n",
    "        self.n_epochs = None\n",
    "        self.train_torch_dataloader = {}\n",
    "        self.valid_torch_dataloader = {}\n",
    "        self.test_torch_dataloader = {}\n",
    "        self.tokenizer = {}\n",
    "        self.optimizer = {}\n",
    "        self.criterion = {}\n",
    "        self.scheduler = {}\n",
    "        self.batch_size = None\n",
    "        self.max_length = None\n",
    "        self.gradient_clipping = None\n",
    "        self.lr_update = None\n",
    "        self.num_workers = None\n",
    "        self.device = \"\"\n",
    "        \n",
    "        # Train results\n",
    "        self.train_all_epoch_labels = []\n",
    "        self.train_all_epoch_losses = []\n",
    "        self.train_all_epoch_targets = []\n",
    "        self.valid_all_epoch_labels = []\n",
    "        self.valid_all_epoch_losses = []\n",
    "        self.valid_all_epoch_targets = []\n",
    "        self.valid_roc_aucs = []\n",
    "        self.train_roc_aucs = []\n",
    "        self.best_metric = - np.inf\n",
    "        self.best_model_state_dict = None\n",
    "        \n",
    "    # Prepare evething and initialize the model\n",
    "    def build(self,\n",
    "              tokenizer_name='distilbert-base-uncased',\n",
    "              model_name='distilbert-base-uncased',\n",
    "              max_length=256,\n",
    "              batch_size=32,\n",
    "              FOLDS=5,\n",
    "              n_epochs=5,\n",
    "              gradient_clipping=False,\n",
    "              amsgrad=True,\n",
    "              model_lr=1e-5,\n",
    "              classifier_lr=1e-3,\n",
    "              lr_update=False,\n",
    "              weight_decay=0,\n",
    "              num_workers=2,\n",
    "              device=\"cuda\"):\n",
    "        \n",
    "        self.is_built = True\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.max_length = max_length\n",
    "        self.lr_update = lr_update\n",
    "        self.gradient_clipping = gradient_clipping\n",
    "        self.num_workers = num_workers\n",
    "        self.device = device\n",
    "        \n",
    "        # Create stritified target from target columns\n",
    "        train[\"stratified_target\"] = train[TARGET_COLUMNS].apply(\n",
    "            lambda x: reduce(lambda x, y: str(x) + str(y), x), axis=1)\n",
    "        small_groups = train[\"stratified_target\"].value_counts()[\n",
    "            train[\"stratified_target\"].value_counts() < FOLDS].index\n",
    "        train.loc[train[\"stratified_target\"].isin(small_groups), \"stratified_target\"] = \"-1\"\n",
    "        \n",
    "        # Cross-validation\n",
    "        stratifier = StratifiedKFold(n_splits=FOLDS,\n",
    "                                     random_state=69,\n",
    "                                     shuffle=True)\n",
    "        folds_ids = [el for el in stratifier.split(train,\n",
    "                                                   train[\"stratified_target\"])]\n",
    "        \n",
    "        # Tokenize text and create dataloader\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        train_torch_dataset = TextDataset(\n",
    "            texts=train.iloc[folds_ids[0][0]][\"clean_comment_text\"].to_list(),\n",
    "            targets=train.iloc[folds_ids[0][0]][TARGET_COLUMNS].values,\n",
    "            dataset_tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        self.train_torch_dataloader = torch.utils.data.DataLoader(\n",
    "            train_torch_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        valid_torch_dataset = TextDataset(\n",
    "            texts=train.iloc[folds_ids[0][1]][\"clean_comment_text\"].to_list(),\n",
    "            targets=train.iloc[folds_ids[0][1]][TARGET_COLUMNS].values,\n",
    "            dataset_tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length,\n",
    "            trim_policy=\"first\"\n",
    "        )\n",
    "        self.valid_torch_dataloader = torch.utils.data.DataLoader(\n",
    "            valid_torch_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(TARGET_COLUMNS), \n",
    "            ignore_mismatched_sizes=True,\n",
    "            max_length=self.max_length\n",
    "        ).to(device)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': self.model.distilbert.parameters(), \"lr\": model_lr},\n",
    "            {'params': self.model.classifier.parameters(), \"lr\": classifier_lr},],\n",
    "            weight_decay=weight_decay,\n",
    "            amsgrad=amsgrad\n",
    "        )\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer=self.optimizer,\n",
    "            num_warmup_steps=int(0.05*len(self.train_torch_dataloader) * self.n_epochs),\n",
    "            num_training_steps=len(self.train_torch_dataloader) * self.n_epochs\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def train(self, verbose=1):\n",
    "        if not self.is_built:\n",
    "            raise Exception('The model was not build and cannot be trained!')\n",
    "        \n",
    "        self.is_trained = True\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # 1.1 Iterate over all train dataset and update model weights\n",
    "            if verbose:\n",
    "                print(f\"Starting Epoch {epoch + 1}\")\n",
    "                print(\"Train phase\")\n",
    "            (train_epoch_labels,\n",
    "            train_epoch_losses,\n",
    "            train_epoch_targets) = self.torch_loop( \n",
    "                mode=\"train\"\n",
    "            )\n",
    "            # 1.2 Compute and print train metrics\n",
    "            if verbose:\n",
    "                print(\"Train metrics\")\n",
    "            _, train_roc_auc = comp_metric(\n",
    "                train_epoch_targets, \n",
    "                train_epoch_labels,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            if verbose:\n",
    "                print(\"Train BCE losses\")\n",
    "            print_losses(train_epoch_losses)\n",
    "            # 2.1 Iterate over all valid dataset and compute predictions\n",
    "            if verbose:\n",
    "                print(\"Valid phase\")\n",
    "            (valid_epoch_labels,\n",
    "            valid_epoch_losses,\n",
    "            valid_epoch_targets) = self.torch_loop(\n",
    "                validation=True, \n",
    "                mode=\"eval\"\n",
    "            )\n",
    "            # 2.2 Compute and print valid metrics\n",
    "            if verbose:\n",
    "                print(\"Valid metrics\")\n",
    "            _, valid_roc_auc = comp_metric(\n",
    "                valid_epoch_targets, \n",
    "                valid_epoch_labels,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            if verbose:\n",
    "                print(\"Valid BCE losses\")\n",
    "            print_losses(valid_epoch_losses, verbose=verbose)\n",
    "            # 3. Update learning rate\n",
    "            if self.lr_update:\n",
    "                self.scheduler.step(valid_roc_auc)\n",
    "            # 4. Save best model\n",
    "            if valid_roc_auc > self.best_metric:\n",
    "                self.best_metric = valid_roc_auc\n",
    "                self.best_model_state_dict = deepcopy(self.model.state_dict())\n",
    "            # 5. Accumulate all stats  \n",
    "            self.train_all_epoch_labels.append(train_epoch_labels)\n",
    "            self.train_all_epoch_losses.append(train_epoch_losses)\n",
    "            self.train_all_epoch_targets.append(train_epoch_targets)\n",
    "            self.valid_all_epoch_labels.append(valid_epoch_labels)\n",
    "            self.valid_all_epoch_losses.append(valid_epoch_losses)\n",
    "            self.valid_all_epoch_targets.append(valid_epoch_targets)\n",
    "            self.valid_roc_aucs.append(valid_roc_auc)\n",
    "            self.train_roc_aucs.append(train_roc_auc)\n",
    "        return self\n",
    "    \n",
    "    def torch_loop(self,\n",
    "                   validation=False,\n",
    "                   mode=\"train\"):\n",
    "        if mode == \"train\":\n",
    "            self.model.train()\n",
    "            dataloader = self.train_torch_dataloader\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            if validation:\n",
    "                dataloader = self.valid_torch_dataloader\n",
    "            else:\n",
    "                dataloader = self.test_torch_dataloader\n",
    "    \n",
    "        all_predicted_label = []\n",
    "        all_losses = []\n",
    "        all_targets = []\n",
    "        with torch.inference_mode(mode=(mode != \"train\")):\n",
    "            for text in tqdm(dataloader):\n",
    "                text = {k:v.to(self.device) for k,v in text.items()}\n",
    "                label = text.pop(\"target\")\n",
    "                if mode == \"train\":\n",
    "                    self.optimizer.zero_grad()\n",
    "                predicted_label = self.model(**text).logits\n",
    "                loss = self.criterion(predicted_label, label)\n",
    "                if mode == \"train\":\n",
    "                    loss.mean().backward()\n",
    "                    if self.gradient_clipping:\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n",
    "                    self.optimizer.step()\n",
    "                    if self.scheduler is not None:\n",
    "                        self.scheduler.step()\n",
    "\n",
    "                all_predicted_label.append(torch.sigmoid(predicted_label.detach()).cpu().numpy())\n",
    "                all_losses.append(loss.detach().cpu().numpy())\n",
    "                all_targets.append(label.detach().cpu().numpy())\n",
    "        all_predicted_label = np.concatenate(all_predicted_label)\n",
    "        all_losses = np.concatenate(all_losses)\n",
    "        all_targets = np.concatenate(all_targets)\n",
    "\n",
    "        return all_predicted_label, all_losses, all_targets\n",
    "    \n",
    "    def evaluate(self, verbose=1):\n",
    "        if not self.is_trained:\n",
    "            raise Exception('The model was not trained and cannot be evaluated!')\n",
    "            return self\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(self.best_model_state_dict)\n",
    "        test_torch_dataset = TextDataset(\n",
    "            texts=test[\"clean_comment_text\"].to_list(),\n",
    "            targets=test[TARGET_COLUMNS].values,\n",
    "            dataset_tokenizer=self.tokenizer,\n",
    "            max_length=self.max_length,\n",
    "            trim_policy=\"first\"\n",
    "        )\n",
    "        self.test_torch_dataloader = torch.utils.data.DataLoader(\n",
    "            test_torch_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        test_labels, test_losses, test_targets = self.torch_loop( \n",
    "            mode=\"eval\"\n",
    "        )\n",
    "        print(\"Test metrics\")\n",
    "        comp_metric(\n",
    "            test_targets, \n",
    "            test_labels,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        print(\"Test BCE losses\")\n",
    "        print_losses(test_losses, verbose=verbose)\n",
    "        \n",
    "        return comp_metric(test_targets,\n",
    "                           test_labels,\n",
    "                           verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T13:45:49.062217Z",
     "iopub.status.busy": "2023-02-27T13:45:49.061895Z",
     "iopub.status.idle": "2023-02-27T18:48:06.589603Z",
     "shell.execute_reply": "2023-02-27T18:48:06.588034Z",
     "shell.execute_reply.started": "2023-02-27T13:45:49.062191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1\n",
      "Train phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [54:03<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Result Roc Auc: 0.9513172580755968\n",
      "Train BCE losses\n",
      "Result BCE loss: 0.07202502340078354\n",
      "Valid phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [04:19<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid metrics\n",
      "Result Roc Auc: 0.9861557119488987\n",
      "Valid BCE losses\n",
      "Result BCE loss: 0.04262630268931389\n",
      "Starting Epoch 2\n",
      "Train phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [54:02<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Result Roc Auc: 0.9886249587017906\n",
      "Train BCE losses\n",
      "Result BCE loss: 0.0385504812002182\n",
      "Valid phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [04:19<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid metrics\n",
      "Result Roc Auc: 0.9877960884632588\n",
      "Valid BCE losses\n",
      "Result BCE loss: 0.04103647172451019\n",
      "Starting Epoch 3\n",
      "Train phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [54:02<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Result Roc Auc: 0.9923040302213552\n",
      "Train BCE losses\n",
      "Result BCE loss: 0.033617328852415085\n",
      "Valid phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [04:19<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid metrics\n",
      "Result Roc Auc: 0.9874316687233299\n",
      "Valid BCE losses\n",
      "Result BCE loss: 0.04154268652200699\n",
      "Starting Epoch 4\n",
      "Train phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [54:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Result Roc Auc: 0.9942330309021455\n",
      "Train BCE losses\n",
      "Result BCE loss: 0.02994224801659584\n",
      "Valid phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [04:18<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid metrics\n",
      "Result Roc Auc: 0.9869368512107836\n",
      "Valid BCE losses\n",
      "Result BCE loss: 0.042650774121284485\n",
      "Starting Epoch 5\n",
      "Train phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3989/3989 [54:02<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics\n",
      "Result Roc Auc: 0.995350926658169\n",
      "Train BCE losses\n",
      "Result BCE loss: 0.027291186153888702\n",
      "Valid phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [04:18<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid metrics\n",
      "Result Roc Auc: 0.9862750219506132\n",
      "Valid BCE losses\n",
      "Result BCE loss: 0.04389544576406479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:38<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics\n",
      "Result Roc Auc: 0.9827018158077921\n",
      "Test BCE losses\n",
      "Result BCE loss: 0.0730869472026825\n",
      "Result Roc Auc: 0.9827018158077921\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "test = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n",
    "test_labels = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\")\n",
    "\n",
    "parameters = {'Sample':[1],'max_length':[512],\n",
    "              'n_epochs':[5],'batch_size':[32],\n",
    "              'gradient_clipping':[False],\n",
    "              'lr_update':[False],'model_lr':[1e-5],\n",
    "              'classifier_lr':[1e-3], 'weight_decay':[0],\n",
    "              'folds':[5]}\n",
    "\n",
    "if os.path.exists('results.csv'):\n",
    "    results = pd.read_csv(\"results.csv\")\n",
    "else:\n",
    "    columns = list(parameters.keys()).append('ROC AUC')\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    \n",
    "train, test = preprocess(train,\n",
    "                         test,\n",
    "                         test_labels,\n",
    "                         parameters['max_length'][0],\n",
    "                         parameters['Sample'][0])\n",
    "\n",
    "model = Model(train, test)\n",
    "model = model.build(max_length=parameters['max_length'][0],\n",
    "                    n_epochs=parameters['n_epochs'][0],\n",
    "                    batch_size=parameters['batch_size'][0],\n",
    "                    gradient_clipping=parameters['gradient_clipping'][0],\n",
    "                    lr_update=parameters['lr_update'][0],\n",
    "                    model_lr=parameters['model_lr'][0],\n",
    "                    classifier_lr=parameters['classifier_lr'][0],\n",
    "                    weight_decay=parameters['weight_decay'][0],\n",
    "                    FOLDS=parameters['folds'][0],\n",
    "                    device=\"cuda\"\n",
    ")\n",
    "model = model.train()\n",
    "_, roc_auc = model.evaluate()\n",
    "\n",
    "parameters.update({'ROC AUC': [roc_auc]})\n",
    "if results.empty:\n",
    "    results = pd.DataFrame(parameters)\n",
    "elif not results.isin(parameters).all(axis=1).any():\n",
    "    results = results.append(parameters, ignore_index=True)\n",
    "results.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T18:48:06.592226Z",
     "iopub.status.busy": "2023-02-27T18:48:06.591822Z",
     "iopub.status.idle": "2023-02-27T18:48:06.615264Z",
     "shell.execute_reply": "2023-02-27T18:48:06.614263Z",
     "shell.execute_reply.started": "2023-02-27T18:48:06.592187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>max_length</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>gradient_clipping</th>\n",
       "      <th>lr_update</th>\n",
       "      <th>model_lr</th>\n",
       "      <th>classifier_lr</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>folds</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.982702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample  max_length  n_epochs  batch_size  gradient_clipping  lr_update  \\\n",
       "0       1         512         5          32              False      False   \n",
       "\n",
       "   model_lr  classifier_lr  weight_decay  folds   ROC AUC  \n",
       "0   0.00001          0.001             0      5  0.982702  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-27T18:48:06.892811Z",
     "iopub.status.busy": "2023-02-27T18:48:06.892346Z",
     "iopub.status.idle": "2023-02-27T18:48:06.899251Z",
     "shell.execute_reply": "2023-02-27T18:48:06.897564Z",
     "shell.execute_reply.started": "2023-02-27T18:48:06.892773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
