{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np \nfrom tqdm import tqdm\nfrom functools import reduce\nfrom copy import deepcopy\nimport matplotlib.pyplot as plt\n\nimport re\nimport torch\nfrom nltk import tokenize\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom transformers import (AutoTokenizer,\n                          AutoModelForSequenceClassification,\n                          get_linear_schedule_with_warmup,\n                          logging)\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nos.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\"\nlogging.set_verbosity_error()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-27T13:45:48.879361Z","iopub.execute_input":"2023-02-27T13:45:48.879868Z","iopub.status.idle":"2023-02-27T13:45:48.891233Z","shell.execute_reply.started":"2023-02-27T13:45:48.879829Z","shell.execute_reply":"2023-02-27T13:45:48.890085Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"TARGET_COLUMNS = [\"toxic\",\n                  \"severe_toxic\",\n                  \"obscene\",\n                  \"threat\",\n                  \"insult\",\n                  \"identity_hate\"]","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:45:48.893645Z","iopub.execute_input":"2023-02-27T13:45:48.894161Z","iopub.status.idle":"2023-02-27T13:45:48.909695Z","shell.execute_reply.started":"2023-02-27T13:45:48.894125Z","shell.execute_reply":"2023-02-27T13:45:48.908627Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def remove_special_chars(text):\n    if isinstance(text, str):\n        # Remove special characters\n        text = re.sub(r'[^\\w\\s]','', text) \n        # Remove non-ASCII characters (such as emojis)\n        text = re.sub(r'[^\\x00-\\x7F]+', '', text) \n        text = \" \".join(tokenize.sent_tokenize(text))\n        text = re.sub(r\"http\\S+\", \"\", text)\n        text = re.sub(r\"\\n+\", \". \", text)\n        for symb in [\"!\", \",\", \":\", \";\", \"?\"]:\n            text = re.sub(rf\"\\{symb}\\.\", symb, text)\n        text = re.sub(\"[^а-яА-Яa-zA-Z0-9!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ё]+\", \" \", text)\n        text = re.sub(r\"#\\S+\", \"\", text)\n        text = text.strip()\n    return text\n\ndef remove_sequential_dots(text):\n    # Collapse sequential dots\n    text = re.sub(\"\\.+\", \".\", text)\n    # Collapse dots separated by whitespaces\n    all_collapsed = False\n    while not all_collapsed:\n        output = re.sub(r\"\\.(( )*)\\.\", \".\", text)\n        all_collapsed = text == output\n        text = output\n    return output\n\ndef remove_stop_words(text, stop_words):\n    tokens = text.split()\n    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text\n\ndef preprocess_text(text, stop_words=stopwords.words('english')):\n    text = remove_special_chars(text)\n    text = remove_sequential_dots(text)\n    text = remove_stop_words(text, stop_words)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:45:48.912778Z","iopub.execute_input":"2023-02-27T13:45:48.913089Z","iopub.status.idle":"2023-02-27T13:45:48.925955Z","shell.execute_reply.started":"2023-02-27T13:45:48.913061Z","shell.execute_reply":"2023-02-27T13:45:48.925026Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def preprocess(train,\n               test,\n               test_labels,\n               max_text_length = 512,\n               sample=1):\n    test = test.merge(test_labels, on=\"id\")\n    \n    train = train.sample(int(train.shape[0]*sample), random_state=69)\n    test = test.sample(int(test.shape[0]*sample), random_state=69)\n    \n    #closed_test = test[(test[TARGET_COLUMNS] == -1).any(axis=1)].reset_index(drop=True)\n    test = test[~(test[TARGET_COLUMNS] == -1).any(axis=1)].reset_index(drop=True)\n    \n    train['clean_comment_text'] = train['comment_text'].apply(preprocess_text)\n    test['clean_comment_text'] = test['comment_text'].apply(preprocess_text)\n    \n    train['clean_comment_text'] = train['clean_comment_text'].str.slice(0, max_text_length)\n    test['clean_comment_text'] = test['clean_comment_text'].str.slice(0, max_text_length)\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:45:48.928657Z","iopub.execute_input":"2023-02-27T13:45:48.929070Z","iopub.status.idle":"2023-02-27T13:45:48.940550Z","shell.execute_reply.started":"2023-02-27T13:45:48.929034Z","shell.execute_reply":"2023-02-27T13:45:48.939653Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def comp_metric(y_true, y_pred, verbose=1):\n    assert y_true.shape == y_pred.shape\n    class_roc_aucs = [roc_auc_score(y_true[:,i], y_pred[:,i]) for i in range(y_pred.shape[1])]\n    if verbose:\n        for ra, tgt_col in zip(class_roc_aucs, TARGET_COLUMNS):\n            if verbose > 1:\n                print(f\"{tgt_col} Roc Auc: {ra}\")\n        print(f\"Result Roc Auc: {np.mean(class_roc_aucs)}\")\n    return class_roc_aucs, np.mean(class_roc_aucs)\n\ndef print_losses(input, verbose=1):\n    if verbose:\n        for cls_idx, cls_name in enumerate(TARGET_COLUMNS):\n            if verbose > 1:\n                print(f\"{cls_name} BCE loss: {input[:,cls_idx].mean()}\")\n        print(f\"Result BCE loss: {input.mean()}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:45:48.941660Z","iopub.execute_input":"2023-02-27T13:45:48.942433Z","iopub.status.idle":"2023-02-27T13:45:48.956410Z","shell.execute_reply.started":"2023-02-27T13:45:48.942392Z","shell.execute_reply":"2023-02-27T13:45:48.955430Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"class TextDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        texts,\n        targets,\n        dataset_tokenizer,\n        max_length,\n        trim_policy=\"random\"\n    ):\n        self.targets = targets\n        self.tokenized_texts =  dataset_tokenizer(texts)\n        self.tokenizer = dataset_tokenizer\n        \n        self.max_length = max_length\n        if trim_policy not in [\"random\", \"first\"]:\n            raise ValueError(f\"{trim_policy} is not valid trim_policy\")\n        self.trim_policy = trim_policy\n        \n    def select_text_subsequance(self, input):\n        input_len = len(input[\"input_ids\"])\n        if input_len < self.max_length:\n            pad_len = self.max_length - input_len\n            return {\n                \"input_ids\": input[\"input_ids\"] + [self.tokenizer.pad_token_id] * pad_len,\n                \"attention_mask\": input[\"attention_mask\"] + [0] * pad_len\n            } \n        elif input_len > self.max_length:\n            if self.trim_policy == \"random\":\n                start = np.random.randint(0, input_len - self.max_length)\n            elif self.trim_policy == \"first\":\n                start = 0\n            return {\n                \"input_ids\": input[\"input_ids\"][start : start + self.max_length - 1] + [self.tokenizer.sep_token_id] ,\n                \"attention_mask\": input[\"attention_mask\"][start : start + self.max_length]\n            }\n        else: \n            return input\n        \n    def __getitem__(self, idx):\n        tokenized = {k:v[idx] for k,v in self.tokenized_texts.items()}\n        tokenized = self.select_text_subsequance(tokenized)\n        tokenized = {k:torch.LongTensor(v) for k,v in tokenized.items()}\n        tokenized[\"target\"] = torch.from_numpy(self.targets[idx]).float()\n        return tokenized\n    \n    def __len__(self):\n        return len(self.targets)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:45:48.958679Z","iopub.execute_input":"2023-02-27T13:45:48.959757Z","iopub.status.idle":"2023-02-27T13:45:48.972828Z","shell.execute_reply.started":"2023-02-27T13:45:48.959711Z","shell.execute_reply":"2023-02-27T13:45:48.971890Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class Model:\n    def __init__(self, train_dataset, test_dataset):\n        # Datasets\n        self.train_dataset = train_dataset\n        self.test_dataset = test_dataset\n        \n        # Model\n        self.model = {}\n        self.is_built = False\n        self.is_trained = False\n        self.n_epochs = None\n        self.train_torch_dataloader = {}\n        self.valid_torch_dataloader = {}\n        self.test_torch_dataloader = {}\n        self.tokenizer = {}\n        self.optimizer = {}\n        self.criterion = {}\n        self.scheduler = {}\n        self.batch_size = None\n        self.max_length = None\n        self.gradient_clipping = None\n        self.lr_update = None\n        self.num_workers = None\n        self.device = \"\"\n        \n        # Train results\n        self.train_all_epoch_labels = []\n        self.train_all_epoch_losses = []\n        self.train_all_epoch_targets = []\n        self.valid_all_epoch_labels = []\n        self.valid_all_epoch_losses = []\n        self.valid_all_epoch_targets = []\n        self.valid_roc_aucs = []\n        self.train_roc_aucs = []\n        self.best_metric = - np.inf\n        self.best_model_state_dict = None\n        \n    # Prepare evething and initialize the model\n    def build(self,\n              tokenizer_name='distilbert-base-uncased',\n              model_name='distilbert-base-uncased',\n              max_length=256,\n              batch_size=32,\n              FOLDS=5,\n              n_epochs=5,\n              gradient_clipping=False,\n              amsgrad=True,\n              model_lr=1e-5,\n              classifier_lr=1e-3,\n              lr_update=False,\n              weight_decay=0,\n              num_workers=2,\n              device=\"cuda\"):\n        \n        self.is_built = True\n        self.batch_size = batch_size\n        self.n_epochs = n_epochs\n        self.max_length = max_length\n        self.lr_update = lr_update\n        self.gradient_clipping = gradient_clipping\n        self.num_workers = num_workers\n        self.device = device\n        \n        # Create stritified target from target columns\n        train[\"stratified_target\"] = train[TARGET_COLUMNS].apply(\n            lambda x: reduce(lambda x, y: str(x) + str(y), x), axis=1)\n        small_groups = train[\"stratified_target\"].value_counts()[\n            train[\"stratified_target\"].value_counts() < FOLDS].index\n        train.loc[train[\"stratified_target\"].isin(small_groups), \"stratified_target\"] = \"-1\"\n        \n        # Cross-validation\n        stratifier = StratifiedKFold(n_splits=FOLDS,\n                                     random_state=69,\n                                     shuffle=True)\n        folds_ids = [el for el in stratifier.split(train,\n                                                   train[\"stratified_target\"])]\n        \n        # Tokenize text and create dataloader\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        train_torch_dataset = TextDataset(\n            texts=train.iloc[folds_ids[0][0]][\"clean_comment_text\"].to_list(),\n            targets=train.iloc[folds_ids[0][0]][TARGET_COLUMNS].values,\n            dataset_tokenizer=self.tokenizer,\n            max_length=self.max_length,\n        )\n        self.train_torch_dataloader = torch.utils.data.DataLoader(\n            train_torch_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            drop_last=True,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n        \n        valid_torch_dataset = TextDataset(\n            texts=train.iloc[folds_ids[0][1]][\"clean_comment_text\"].to_list(),\n            targets=train.iloc[folds_ids[0][1]][TARGET_COLUMNS].values,\n            dataset_tokenizer=self.tokenizer,\n            max_length=self.max_length,\n            trim_policy=\"first\"\n        )\n        self.valid_torch_dataloader = torch.utils.data.DataLoader(\n            valid_torch_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            drop_last=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n        \n        # Initialize model\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=len(TARGET_COLUMNS), \n            ignore_mismatched_sizes=True,\n            max_length=self.max_length\n        ).to(device)\n        \n        # Hyperparameters\n        self.criterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n        self.optimizer = torch.optim.AdamW([\n            {'params': self.model.distilbert.parameters(), \"lr\": model_lr},\n            {'params': self.model.classifier.parameters(), \"lr\": classifier_lr},],\n            weight_decay=weight_decay,\n            amsgrad=amsgrad\n        )\n        self.scheduler = get_linear_schedule_with_warmup(\n            optimizer=self.optimizer,\n            num_warmup_steps=int(0.05*len(self.train_torch_dataloader) * self.n_epochs),\n            num_training_steps=len(self.train_torch_dataloader) * self.n_epochs\n        )\n        return self\n    \n    def train(self, verbose=1):\n        if not self.is_built:\n            raise Exception('The model was not build and cannot be trained!')\n        \n        self.is_trained = True\n        for epoch in range(self.n_epochs):\n            # 1.1 Iterate over all train dataset and update model weights\n            if verbose:\n                print(f\"Starting Epoch {epoch + 1}\")\n                print(\"Train phase\")\n            (train_epoch_labels,\n            train_epoch_losses,\n            train_epoch_targets) = self.torch_loop( \n                mode=\"train\"\n            )\n            # 1.2 Compute and print train metrics\n            if verbose:\n                print(\"Train metrics\")\n            _, train_roc_auc = comp_metric(\n                train_epoch_targets, \n                train_epoch_labels,\n                verbose=verbose\n            )\n            if verbose:\n                print(\"Train BCE losses\")\n            print_losses(train_epoch_losses)\n            # 2.1 Iterate over all valid dataset and compute predictions\n            if verbose:\n                print(\"Valid phase\")\n            (valid_epoch_labels,\n            valid_epoch_losses,\n            valid_epoch_targets) = self.torch_loop(\n                validation=True, \n                mode=\"eval\"\n            )\n            # 2.2 Compute and print valid metrics\n            if verbose:\n                print(\"Valid metrics\")\n            _, valid_roc_auc = comp_metric(\n                valid_epoch_targets, \n                valid_epoch_labels,\n                verbose=verbose\n            )\n            if verbose:\n                print(\"Valid BCE losses\")\n            print_losses(valid_epoch_losses, verbose=verbose)\n            # 3. Update learning rate\n            if self.lr_update:\n                self.scheduler.step(valid_roc_auc)\n            # 4. Save best model\n            if valid_roc_auc > self.best_metric:\n                self.best_metric = valid_roc_auc\n                self.best_model_state_dict = deepcopy(self.model.state_dict())\n            # 5. Accumulate all stats  \n            self.train_all_epoch_labels.append(train_epoch_labels)\n            self.train_all_epoch_losses.append(train_epoch_losses)\n            self.train_all_epoch_targets.append(train_epoch_targets)\n            self.valid_all_epoch_labels.append(valid_epoch_labels)\n            self.valid_all_epoch_losses.append(valid_epoch_losses)\n            self.valid_all_epoch_targets.append(valid_epoch_targets)\n            self.valid_roc_aucs.append(valid_roc_auc)\n            self.train_roc_aucs.append(train_roc_auc)\n        return self\n    \n    def torch_loop(self,\n                   validation=False,\n                   mode=\"train\"):\n        if mode == \"train\":\n            self.model.train()\n            dataloader = self.train_torch_dataloader\n        else:\n            self.model.eval()\n            if validation:\n                dataloader = self.valid_torch_dataloader\n            else:\n                dataloader = self.test_torch_dataloader\n    \n        all_predicted_label = []\n        all_losses = []\n        all_targets = []\n        with torch.inference_mode(mode=(mode != \"train\")):\n            for text in tqdm(dataloader):\n                text = {k:v.to(self.device) for k,v in text.items()}\n                label = text.pop(\"target\")\n                if mode == \"train\":\n                    self.optimizer.zero_grad()\n                predicted_label = self.model(**text).logits\n                loss = self.criterion(predicted_label, label)\n                if mode == \"train\":\n                    loss.mean().backward()\n                    if self.gradient_clipping:\n                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n                    self.optimizer.step()\n                    if self.scheduler is not None:\n                        self.scheduler.step()\n\n                all_predicted_label.append(torch.sigmoid(predicted_label.detach()).cpu().numpy())\n                all_losses.append(loss.detach().cpu().numpy())\n                all_targets.append(label.detach().cpu().numpy())\n        all_predicted_label = np.concatenate(all_predicted_label)\n        all_losses = np.concatenate(all_losses)\n        all_targets = np.concatenate(all_targets)\n\n        return all_predicted_label, all_losses, all_targets\n    \n    def evaluate(self, verbose=1):\n        if not self.is_trained:\n            raise Exception('The model was not trained and cannot be evaluated!')\n            return self\n        \n        # Load best model\n        self.model.load_state_dict(self.best_model_state_dict)\n        test_torch_dataset = TextDataset(\n            texts=test[\"clean_comment_text\"].to_list(),\n            targets=test[TARGET_COLUMNS].values,\n            dataset_tokenizer=self.tokenizer,\n            max_length=self.max_length,\n            trim_policy=\"first\"\n        )\n        self.test_torch_dataloader = torch.utils.data.DataLoader(\n            test_torch_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            drop_last=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n        test_labels, test_losses, test_targets = self.torch_loop( \n            mode=\"eval\"\n        )\n        print(\"Test metrics\")\n        comp_metric(\n            test_targets, \n            test_labels,\n            verbose=verbose\n        )\n        \n        print(\"Test BCE losses\")\n        print_losses(test_losses, verbose=verbose)\n        \n        return comp_metric(test_targets,\n                           test_labels,\n                           verbose=verbose)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:45:49.015592Z","iopub.execute_input":"2023-02-27T13:45:49.016323Z","iopub.status.idle":"2023-02-27T13:45:49.058917Z","shell.execute_reply.started":"2023-02-27T13:45:49.016284Z","shell.execute_reply":"2023-02-27T13:45:49.057925Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ntest_labels = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\")\n\nparameters = {'Sample':[1],'max_length':[512],\n              'n_epochs':[5],'batch_size':[32],\n              'gradient_clipping':[False],\n              'lr_update':[False],'model_lr':[1e-5],\n              'classifier_lr':[1e-3], 'weight_decay':[0],\n              'folds':[5]}\n\nif os.path.exists('results.csv'):\n    results = pd.read_csv(\"results.csv\")\nelse:\n    columns = list(parameters.keys()).append('ROC AUC')\n    results = pd.DataFrame(columns=columns)\n    \ntrain, test = preprocess(train,\n                         test,\n                         test_labels,\n                         parameters['max_length'][0],\n                         parameters['Sample'][0])\n\nmodel = Model(train, test)\nmodel = model.build(max_length=parameters['max_length'][0],\n                    n_epochs=parameters['n_epochs'][0],\n                    batch_size=parameters['batch_size'][0],\n                    gradient_clipping=parameters['gradient_clipping'][0],\n                    lr_update=parameters['lr_update'][0],\n                    model_lr=parameters['model_lr'][0],\n                    classifier_lr=parameters['classifier_lr'][0],\n                    weight_decay=parameters['weight_decay'][0],\n                    FOLDS=parameters['folds'][0],\n                    device=\"cuda\"\n)\nmodel = model.train()\n_, roc_auc = model.evaluate()\n\nparameters.update({'ROC AUC': [roc_auc]})\nif results.empty:\n    results = pd.DataFrame(parameters)\nelif not results.isin(parameters).all(axis=1).any():\n    results = results.append(parameters, ignore_index=True)\nresults.to_csv(\"results.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T13:45:49.061895Z","iopub.execute_input":"2023-02-27T13:45:49.062217Z","iopub.status.idle":"2023-02-27T18:48:06.589603Z","shell.execute_reply.started":"2023-02-27T13:45:49.062191Z","shell.execute_reply":"2023-02-27T18:48:06.588034Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Starting Epoch 1\nTrain phase\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3989/3989 [54:03<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train metrics\nResult Roc Auc: 0.9513172580755968\nTrain BCE losses\nResult BCE loss: 0.07202502340078354\nValid phase\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 998/998 [04:19<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Valid metrics\nResult Roc Auc: 0.9861557119488987\nValid BCE losses\nResult BCE loss: 0.04262630268931389\nStarting Epoch 2\nTrain phase\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3989/3989 [54:02<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train metrics\nResult Roc Auc: 0.9886249587017906\nTrain BCE losses\nResult BCE loss: 0.0385504812002182\nValid phase\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 998/998 [04:19<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Valid metrics\nResult Roc Auc: 0.9877960884632588\nValid BCE losses\nResult BCE loss: 0.04103647172451019\nStarting Epoch 3\nTrain phase\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3989/3989 [54:02<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train metrics\nResult Roc Auc: 0.9923040302213552\nTrain BCE losses\nResult BCE loss: 0.033617328852415085\nValid phase\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 998/998 [04:19<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Valid metrics\nResult Roc Auc: 0.9874316687233299\nValid BCE losses\nResult BCE loss: 0.04154268652200699\nStarting Epoch 4\nTrain phase\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3989/3989 [54:05<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train metrics\nResult Roc Auc: 0.9942330309021455\nTrain BCE losses\nResult BCE loss: 0.02994224801659584\nValid phase\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 998/998 [04:18<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Valid metrics\nResult Roc Auc: 0.9869368512107836\nValid BCE losses\nResult BCE loss: 0.042650774121284485\nStarting Epoch 5\nTrain phase\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3989/3989 [54:02<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train metrics\nResult Roc Auc: 0.995350926658169\nTrain BCE losses\nResult BCE loss: 0.027291186153888702\nValid phase\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 998/998 [04:18<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Valid metrics\nResult Roc Auc: 0.9862750219506132\nValid BCE losses\nResult BCE loss: 0.04389544576406479\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [08:38<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test metrics\nResult Roc Auc: 0.9827018158077921\nTest BCE losses\nResult BCE loss: 0.0730869472026825\nResult Roc Auc: 0.9827018158077921\n","output_type":"stream"}]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2023-02-27T18:48:06.591822Z","iopub.execute_input":"2023-02-27T18:48:06.592226Z","iopub.status.idle":"2023-02-27T18:48:06.615264Z","shell.execute_reply.started":"2023-02-27T18:48:06.592187Z","shell.execute_reply":"2023-02-27T18:48:06.614263Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"   Sample  max_length  n_epochs  batch_size  gradient_clipping  lr_update  \\\n0       1         512         5          32              False      False   \n\n   model_lr  classifier_lr  weight_decay  folds   ROC AUC  \n0   0.00001          0.001             0      5  0.982702  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sample</th>\n      <th>max_length</th>\n      <th>n_epochs</th>\n      <th>batch_size</th>\n      <th>gradient_clipping</th>\n      <th>lr_update</th>\n      <th>model_lr</th>\n      <th>classifier_lr</th>\n      <th>weight_decay</th>\n      <th>folds</th>\n      <th>ROC AUC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>512</td>\n      <td>5</td>\n      <td>32</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.00001</td>\n      <td>0.001</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0.982702</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(\"a\")","metadata":{"execution":{"iopub.status.busy":"2023-02-27T18:48:06.892346Z","iopub.execute_input":"2023-02-27T18:48:06.892811Z","iopub.status.idle":"2023-02-27T18:48:06.899251Z","shell.execute_reply.started":"2023-02-27T18:48:06.892773Z","shell.execute_reply":"2023-02-27T18:48:06.897564Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"a\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}